\chapter{Introduction}


\section{Summary of research contributions}

\begin{itemize}
\item \cref{sec:cpeg1} presents the \emph{OptNet} architecture
  that shows how to use constrained convex optimization
  as a layer in an end-to-end architecture.
  \begin{itemize}
  \item \cref{sec:optnet:formulation} presents the
    formulation of these architectures and shows how
    backpropagation can be done in them by
    implicitly differentiating the KKT conditions.
  \item \cref{sec:optnet:rep-power} studies the
    representational power of these architectures and proves
    how they can represent any piecewise linear function
    including the ReLU.
  \item \cref{sec:optnet:qp-solver} presents our
    efficient QP solver for these layers and
    \cref{sec:optnet:qp-solver-grads}
    shows how we can compute the backwards pass with
    almost no computational overhead.
  \item \cref{sec:icnn:exp} shows empirical results
    that uses OptNet for a synthetic denoising task
    and to learn the rules of the Sudoku game.
  \end{itemize}
\item \cref{sec:icnn} presents the \emph{input-convex neural
  network} architecture.
  \begin{itemize}
  \item \cref{sec:icnn:inf} discusses efficient inference
    techniques for these architectures.
    We propose a new inference technique called the Bundle-Entropy
    method in \cref{sec:icnn:inf:be}.
  \item \cref{sec:icnn:learning} discusses efficient
    learning techniques for these architecture.
  \item \cref{sec:icnn:exps} shows empirical results applying
    ICNNs to structured prediction, data imputation, and
    continuous-action Q learning.
  \end{itemize}
\end{itemize}

\vspace{4mm}
The remaining portions discuss applications and
extensions of OptNet.
\begin{itemize}
\item \cref{sec:empc} presents our
  \emph{differentiable model predictive control} (MPC) work
  as a step towards leveraging MPC as a differentiable
  policy class for reinforcement learning in continuous
  state-action spaces.
  \begin{itemize}
  \item \cref{sec:empc:diff-lqr} shows how to efficiently
    differentiate through the Linear Quadratic Regulator (LQR)
    by solving another LQR problem.
    This result comes from implicitly differentiating the KKT
    conditions of LQR and interpreting the resulting system
    as solving another LQR problem.
  \item \cref{sec:empc:diff-mpc} shows how to differentiate
    through non-convex MPC problems by differentiating through the
    fixed point obtained when solving the MPC problem with
    sequential quadratic programming (SQP).
  \item \cref{sec:empc:exps} shows our empirical results
    using imitation learning in the pendulum and cartpole
    environments.
    Notably, we show why doing end-to-end learning with
    a controller is important in tasks when the expert
    is non-realizable.
  \end{itemize}
  \newpage
\item \cref{sec:lml} presents the Limited Multi-Layer Projection
  (LML) layer for top-$k$ learning problems.
  \begin{itemize}
  \item \cref{sec:lml:lml} introduces the LML projection problem
    that we study.
  \item \cref{sec:lml:lml:efficient} shows how to efficiently
    solve the LML projection problem by solving the dual
    with a parallel bracketed root-finding method.
  \item \cref{sec:lml:topk} presents how to maximize
    the top-$k$ recall with the LML layer.
  \item \cref{sec:lml:ex} shows our empirical results on
    top-$k$ image classification and scene graph generation.
  \end{itemize}
\item \cref{sec:cvxpyth} shows how to make differentiable
  \cvxpy optimization layers by differentiating through
  the internal transformations and internal cone program solver.
  This enables rapid prototyping of all of the convex
  optimization-based modeling methods we consider in this thesis.
  \begin{itemize}
  \item \cref{sec:cvxpyth:diff-cp} shows how to differentiate
    cone programs (including non-polyhedral cone programs)
    by implicitly differentiating the residual map of
    Minty's parameterization of the homogeneous
    self-dual embedding.
  \item \cref{sec:cvxpyth:examples} shows examples of using our
    package to implement optimization layers for
    the ReLU, sigmoid, softmax; projections onto polyhedral
    and ellipsoidal sets; and the OptNet QP.
  \end{itemize}
\end{itemize}

\newpage
\section{Summary of open source contributions}
The code and experiments developed for this thesis
are free and open-source:

\begin{itemize}
\item \url{https://github.com/locuslab/icnn}:
  TensorFlow experiments for the input-convex neural networks
  work presented in \cref{sec:icnn}.
\item \url{https://locuslab.github.io/qpth/} and
  \url{https://github.com/locuslab/qpth}:
  A stand-alone PyTorch library for the OptNet QP layers presented
  in \cref{sec:optnet}.
\item \url{https://github.com/locuslab/optnet}:
  PyTorch experiments for the OptNet work
  presented in \cref{sec:optnet}.
\item \url{https://locuslab.github.io/mpc.pytorch}
  and \url{https://github.com/locuslab/mpc.pytorch}:
  A stand-alone PyTorch library for the differentiable
  model predictive control approach presented in
  \cref{sec:empc}.
\item \url{https://github.com/locuslab/differentiable-mpc}:
  PyTorch experiments for the differentiable MPC work
  presented in \cref{sec:empc}.
\end{itemize}

\vspace{5mm}
\noindent
I have also created the following open source
projects during my Ph.D.:
\begin{itemize}
\item \url{https://github.com/bamos/block}:
  An intelligent block matrix library for numpy, PyTorch, and beyond.
\item \url{https://github.com/bamos/dcgan-completion.tensorflow}:
  Image Completion with Deep Learning in TensorFlow.
\item \url{https://github.com/cmusatyalab/openface}:
  Face recognition with deep neural networks.
\item \url{https://github.com/bamos/densenet.pytorch}:
  A PyTorch implementation of DenseNet.
\end{itemize}

\newpage
\section{Summary of publications}
\newcommand{\fcite}[1]{
  \begin{leftbar}
  \begin{quote}%
    \citep{#1} \fullcite{#1}
  \end{quote}
  \end{leftbar}}

\noindent The content of \cref{sec:cpeg1} appears in:
\fcite{tracy2022c}
\vspace{5mm}

\noindent The content of \cref{sec:cpeg2} appears in:
\fcite{tracy2023}
\vspace{5mm}

\noindent The content of \cref{sec:wigglesat} appears in:
\fcite{tracy2022a}
\vspace{5mm}

\noindent
\textbf{Non-thesis research:}
I have also pursued the following research
directions during my Ph.D.~studies.
These are excluded from the remainder of this thesis.

\begin{leftbar}
\begin{quote}%
  \citep{amos2018learning} \fullcite{amos2018learning} \\[5mm]
  \citep{amos2016openface} \fullcite{amos2016openface} \\
  \textbf{Available online at:}
  \url{https://cmusatyalab.github.io/openface} \\
\end{quote}
\end{leftbar}

\vspace{7mm}
\noindent
I have also contributed to the following
publications as a non-primary author.

\begin{leftbar}
\begin{quote}%
  \fullcite{donti2017task} \\[5mm]
  \fullcite{zhao2016collapsed} \\[5mm]
  \fullcite{chen2017empirical} \\[5mm]
  \fullcite{chen2015early} \\[5mm]
  \fullcite{davies2016privacy} \\[5mm]
  \fullcite{wang2017scalable} \\[5mm]
  \fullcite{hu2014case} \\[5mm]
  \fullcite{satyanarayanan2015edge} \\[5mm]
  \fullcite{gao2015cloudlets} \\[5mm]
  \fullcite{ha2017you} \\[5mm]
  \fullcite{hu2016quantifying}
\end{quote}
\end{leftbar}



%%% Local Variables:
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "../thesis"
%%% End: